## Qs now
- **Finetune**
	- Local finetuning (if better than context and embedding)
		- What's academic best practice when finetuning local models?
	- If not: Covering api-costs through CBS
- **Lit-review** recommendations
- Prototype:
	- Human messages --> LLM agent
	- Framework for giving agency through persona adoption from messages between 2 (target individual and other person)
- Post-prototype
	- Partner orgs for more data
	- **Theoretical framework**
		- Market research methodology
			- Conjoint analysis
		- **Survey** recommendations?
			- Psychological frameworks perceived as ==normative enough ==for DIGI-department
				- MBTI? --> Psychological archetypes
	- Would u classify this as an **explorative**, testing, or â€¦. Research project?

### Model
- Proprietary:
	- ==gpt-3.5-turbo-0125==
		- Context=16k
		- 24th --> 18th best [LLM arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)
		- $/1M tokens
			- .5 I
			- 1.5 O
		- Best OpenAI-model that can be fine-tuned
- Open weights
	- Run locally
		- ==Mistral 7B Instruct V0.2==
			- Context = 32K
			- 32th --> 27th
		- Starling-LM-7B-alpha
			- Context 8K
			- 26th --> 25th
	- Run in [ucloud](https://cloud.sdu.dk/app/applications/overview/)
		- ==Qwen1.5-72B-Chat==
			- context=32k
			- 10th --> 8th
		- WizardLM-70B-v1.0
			- context =4K
			- 20th --> 18th

---

## Future Meetings Optimization
- Up-2-date TOC sheet?
- KanBan access?
- Weekly catch up?
	- 1 hour each time?
	- How do u think is best
